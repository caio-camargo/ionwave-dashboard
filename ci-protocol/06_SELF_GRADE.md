# 06 — SELF-GRADE
**Final quality gate before delivery. Catches what slipped through.**

---

## PURPOSE

This is the last step before the deliverable reaches the human. Its job is to catch:
- Missing confidence tags
- Upgrade paths that are vague or incomplete
- Scorecard that doesn't match the actual grades
- Gaps that were identified but not documented
- Claims that sound confident but are actually D-grade

---

## GRADING RUBRIC

Score each dimension. Be honest — this is internal.

### Dimension 1: Evidence Coverage
*Did we find something for every metric we were looking for?*

| Score | Meaning |
|-------|---------|
| 5 | Every metric has at least one source. No "we couldn't find anything" gaps. |
| 4 | Most metrics covered. 1-2 metrics at D-grade with upgrade paths. |
| 3 | Several metrics missing or D-grade. Upgrade paths documented but substantial. |
| 2 | Major sections are skeleton-only. More upgrade paths than actual data. |
| 1 | Mostly guesses. Output is a framework, not intelligence. |

### Dimension 2: Confidence Honesty
*Are grades accurate, or did we inflate them?*

| Score | Meaning |
|-------|---------|
| 5 | Every grade is defensible against the source hierarchy in 02_RESEARCH_PHASE.md. No inflation. |
| 4 | Grades are mostly accurate. Maybe 1-2 generous interpretations. |
| 3 | Several grades feel generous. "B" assigned to things that are really "C." |
| 2 | Systematic inflation. Output reads as more confident than evidence supports. |
| 1 | Grades are meaningless. Everything is called "B" regardless of source quality. |

### Dimension 3: Upgrade Path Quality
*Are below-A items' upgrade paths actually actionable?*

| Score | Meaning |
|-------|---------|
| 5 | Every upgrade path specifies: exact task, time estimate, expected grade outcome. A stranger could execute it. |
| 4 | Most upgrade paths are actionable. 1-2 are vague ("get better data"). |
| 3 | Half the upgrade paths are vague. Need human to interpret what "verify this" means. |
| 2 | Upgrade paths are mostly "check this source" without specifics. |
| 1 | No upgrade paths, or upgrade paths are just "human should look into this." |

### Dimension 4: Scorecard Accuracy
*Does the scorecard match what's actually in the deliverable?*

| Score | Meaning |
|-------|---------|
| 5 | Scorecard grades match View 1 grades exactly. Summary accurately characterizes the analysis. Shopping list is complete and prioritized. |
| 4 | Minor discrepancies (1-2 grades off by one level). Shopping list mostly complete. |
| 3 | Scorecard tells a different story than the actual analysis. |
| 2 | Scorecard is optimistic vs. reality. |
| 1 | Scorecard is disconnected from the deliverable. |

### Dimension 5: Strategic Value
*Would this actually inform a strategic decision?*

| Score | Meaning |
|-------|---------|
| 5 | Reading this changes how you think about the competitive landscape. Actionable implications are clear. |
| 4 | Useful for strategic planning. A few insights that wouldn't be obvious without this analysis. |
| 3 | Informational but not particularly insightful. Confirms what was already suspected. |
| 2 | Mostly facts without interpretation. Reader has to do the strategic thinking themselves. |
| 1 | No strategic value beyond "this competitor exists." |

---

## OVERALL GRADE

| Avg Score | Protocol Grade | Meaning |
|-----------|---------------|---------|
| 4.5-5.0 | A | Ready for delivery. Investor-grade quality (even if individual metrics are C/D — the *process* is A-grade) |
| 3.5-4.4 | B | Ready for delivery with a note. Flag 1-2 areas for human attention. |
| 2.5-3.4 | C | Needs revision before delivery. Identify lowest-scoring dimension, fix it. |
| < 2.5 | D | Do not deliver. Something fundamental is wrong. Re-examine. |

**Key principle:** The protocol grade is about process quality, not data quality. A D-grade deliverable (all estimates are guesses) can be an A-grade protocol run if every guess is clearly labeled, has an upgrade path, and the scorecard accurately represents the situation. The protocol's job is honesty, not omniscience.

---

## PRE-DELIVERY CHECKLIST

Run through this before delivering:

- [ ] **Tags:** Every factual claim in View 1 has a confidence tag
- [ ] **No naked claims:** No sentence states something as fact without a grade
- [ ] **Upgrade paths exist** for every below-A item
- [ ] **Upgrade paths are actionable:** Each one specifies what to do, not just "verify"
- [ ] **Scorecard matches reality:** Grades in scorecard = grades in View 1
- [ ] **Shopping list is prioritized:** Highest-impact upgrades first
- [ ] **Gaps are documented:** Every UNRESOLVED tension from persona dialogue appears in View 2
- [ ] **Benchmark anomalies are noted:** Strategic notes in View 1 reflect anomalies from 03
- [ ] **Persona dialogue summary exists:** Saved as process document
- [ ] **State tracker is updated:** PHASE = COMPLETE, all fields current

---

## COMMON SELF-GRADING MISTAKES

1. **Grading the analysis A because it's thorough, not because it's accurate.** Thoroughness ≠ confidence. A thorough analysis of guesses is still D-grade on evidence.

2. **Forgetting upgrade paths for "obvious" gaps.** If SEMrush data would upgrade traffic from C to A, that needs to be in the shopping list even if it feels obvious.

3. **Scorecard summary being aspirational.** The summary should describe what the analysis IS, not what it COULD be after upgrades.

4. **Not counting benchmark anomalies as strategic value.** If the benchmark check surfaced an exploitable weakness, that's strategic value even if the underlying data is C-grade.

5. **Treating the persona dialogue summary as optional.** It's not. It's the evidence that the analysis was stress-tested. Without it, we can't distinguish a rigorous analysis from a confident guess.
